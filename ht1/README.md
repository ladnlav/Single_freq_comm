# Отчет о домешнем задании по канальному кодированию
## Первая часть: разогрев

В данной части задания предлагалось вычислить синдром $s=x*G$, где $x$ - исходный сигнал, $G$ - порождающая матрица. После чего нужно было инвертировать один бит в закодированной последовательности и снова вычислить синдром. При этом, домножая на синдром $s$ на транспонированную проверочную матрицу $H$ мы должны получать 0: $s*H^T=0$. как можно видеть, этого не происходит, когда мы умножаем последовательность с инвертированным битом.

![Alt-text](<images/part1.png>)

## Вторая часть: сравнение количества ошибок с LDPC кодированием и без него

Тип IQ-созвездия при расчетах - _16QAM_. Но реализованный код позволяет использовать любое из следующих созвездий: _BPSK, QPSK, 8PSK, 16QAM_.

### LDPC vs vanilla
![Alt-text](<graphs/2. BER(Eb_N0).png>)
![Alt-text](<graphs/3. GAIN(Eb_N0).png>)

Как можно видеть из графиков выше, последовательность, закодированная с помощью LDPC, обладает лучшей помехоустойчивостью, чем та же последовательность без какого-либо кодирования. И максимальный gain равен примерно 27 dB. 

### Additionally: Exact vs approximate LLR
![Alt-text](<images/part2_1.png>)
![Alt-text](<images/part2_2.png>)

В рамках этого задания был реализован soft-demapper, рассчитывающий Log-Likelihood Ratio (LLR). Использовались два метода расчета LLR: точный (exact) и приблизительный (approximate).

![Alt-text](<graphs/4. Exact vs App LLR.png>)
![Alt-text](<graphs/8. Time exact vs app.png>)

Как можно видеть из графиков выше, BER у обоих методов совпадает с достаточной точностью (практически полностью), а вот по времени приблизительный метод расчета быстрее __примерно на 0,1 миллисекунду__. В силу этого, далее в программе везде был использован приблизительный метод расчета LLR для ускорения вычислений.

## Третья часть: стандартный LDPC c разным количеством итераций декодирования

```
Дополнительно: Исходный код был распараллелен.
```
В этой части предлагалось сравнить perfomance LDPC кодирования с разным количеством итераций во время декодирования. Изначально предлагалось сравнить 5 и 20 итераций, я добавил ещё 10 и 30. Резульаты можно видеть на графике.

![Alt-text](<graphs/7. Different amount of LDPC iterations.png>)

Очевидно, что лучший BER у большего количества итераций.

## Четвёртая часть: разные алгоритмы LPDC: MinSum vs Belief Propagation

В данной части задания нужно было сравнить количество ошибок двух алгоритмов, а также их время работы. Результаты - на графиках.

![Alt-text](<graphs/5. MinSum vs BP.png>)
![Alt-text](<graphs/6. Time MinSum vs BP.png>)

Общая тендеция такова, что MinSum алгоритм несколько уступает BP в показателях BER до определённых значений $\frac{E_b}{N_0}$, зато его время работы при низких значениях $\frac{E_b}{N_0}$ приблизительно на 1.5 миллисекунды меньше. Отсюда вывод, что BP - это более долгий алгортим, зато точный. В то время как MinSum - быстрый относительно BP, но совсем немного уступает во времени работы.
